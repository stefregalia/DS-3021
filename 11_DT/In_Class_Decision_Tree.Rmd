---
title: "In Class DT"
author: "Brian Wright"
date: "December 7, 2017"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Congrats! You just graduated UVA's MSDS program and got a job working at the 
Treasury Department. In partnership with Congress members the Treasury has been 
ask to come up with innovative ways to create tax policy. In doing so they 
want to be able to reliably predict whether American workers make more than 
$50,000 a year and also which variables seem to be most contributing 
to predicting this outcome. 

You would like to be able to explain the model to the mere mortals 
around you but need a fairly robust and flexible approach so you've 
chosen to use decision trees to get started and will possibly move 
to a ensemble model if needed. 

In doing so, similar to  great data scientists of the past 
you remembered the excellent education provided 
to you at UVA in a undergrad data science course and have outline 
20ish steps that will need to be undertaken to complete this task 
(you can add more or combine if needed).  As always, you will need 
to make sure to #comment your work heavily and render the results in 
a clear report (knitted). 


```{r}
url <- "http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"

xx <- readr::read_csv(url)

View(xx)
```


 Footnotes: 
-	Some of the steps will not need to be repeated for the second model, 
use your judgment
-	You can add or combine steps if needed
-	Also, remember to try several methods during evaluation and always be 
mindful of how the model will be used in practice.
- Make sure all your variables are the correct type (factor, character, etc.)



```{r}
#1 Load the data, check for missing data and ensure the labels are correct.   

```

```{r}
#2 Ensure all the variables are classified correctly including the target 
# variable
```

```{r}
#3 Don't check for correlated variables....because it doesn't matter with 
# Decision Trees...the make local greedy decisions. 
```


```{r}
#4 Guess what, you also don't need to standardize the data, 
#because DTs don't give a ish, they make local decisions...keeps getting easier

```


```{r}
#5 Ok now determine the baserate or prevalence for the classifier, 
# what does this number mean?  

```

```{r}
#6 Split your data into test, validation and train. 
```


```{r}
#7 Build your model using the training data and default settings in caret, 
# double check to make sure you are using a cross-validation training approach
```

```{r}
#8 View the results, what is the most important variable for the tree? 
```

```{r}
#9 Plot the output of the model to see the tree visually 
```

```{r}
#10 Use the validation set and the predict function with your model to the
# estimate the target variable 
```

```{r}
#11 Compare the predicted values to those of the actual by generating a 
# matrix ("by-hand").
```

```{r}
#14 Generate, "by-hand", the hit rate and detection rate and compare the 
# detection rate to your original baseline rate. How did your model work?
```

```{r}
#15 Use the the confusion matrix function to check a variety of metrics 
# and comment on the metric that might be best for this type of analysis.  
```

```{r}
#16 Generate a ROC and AUC output, interpret the results
```

```{r}
#17 Use the predict function to generate percentages, then select several 
# different threshold levels using the confusion matrix function and 
# interpret the results? What metric should you be trying to optimize. 
```

```{r}
#18 Try adjusting several other hyperparameters via the built in control function
# in caret, does the model quality improve? 
```

```{r}
#19 Once you are confident that your model is not improving, via changes 
# implemented on the training set while exploring evaluation output from
#the validation set, predict with the test set and evaluate the model.
```

```{r}
#20 Summarize what you learned along the way and make recommendations to the 
#world on how this could be used moving forward, being careful not to over promise. 
```



