---
title: "Decision Tree Lab"
author: "Madeleine Jones"
date: "November 10, 2021"
output: 
  html_document:
    toc: TRUE
    toc_float: TRUE
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
library(rio)
library(plyr)
library(tidyverse)
library(rpart)
library(psych)
library(pROC)
#install.packages("rpart.plot")
library(rpart.plot)
#install.packages("rattle")
library(rattle)
library(caret)
library(C50) #Need this to pass into caret 
library(mlbench)
library(MLmetrics)
library(ROCR)
```

Congrats! You just graduated UVA's MSDS program and got a job working at the 
Treasury Department. In partnership with Congress members the Treasury has been 
ask to come up with innovative ways to create tax policy. In doing so they 
want to be able to reliably predict whether American workers make more than 
$50,000 a year and also which variables seem to be most contributing 
to predicting this outcome. 

You would like to be able to explain the model to the mere mortals 
around you but need a fairly robust and flexible approach so you've 
chosen to use decision trees to get started and will possibly move 
to a ensemble model if needed. 

In doing so, similar to  great data scientists of the past 
you remembered the excellent education provided 
to you at UVA in a undergrad data science course and have outline 
20ish steps that will need to be undertaken to complete this task 
(you can add more or combine if needed).  As always, you will need 
to make sure to #comment your work heavily. 


## Step 1: Load the Data, Check for Missing Data, and Ensure Labels are Correct
```{r}
url <- "http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data" # save url where data is 

names <- c("age","workclass","fnlwgt","education","educationnum","maritalstatus","occupation","relationship","race","sex","capitalgain","capitalloss","hoursperweek","nativecountry", "salary")  # list of column names

xx <- readr::read_csv(url, col_names = names)  # read the csv from the url and use the column names vector as the column names for the csv

# replace the question marks in the work class, occupation, and native country columns with NAs so that they can be more easily removed
xx <- gsub("?",NA,xx, fixed = TRUE)
View(xx)
sum(is.na(xx))

table(xx)
#xx$occupation <- gsub("?",NA,xx$occupation, fixed = TRUE)
#xx$`nativecountry` <- gsub("?",NA,xx$`nativecountry`, fixed = TRUE)

# relabel the salary entries as string with known characters 
xx$salary <- fct_collapse(xx$salary,
                            below50K=c("<=50K"),
                            above50K=c(">50K"))

data <- xx[complete.cases(xx), ]  # subset only the observations with no NAs (complete observations) and save as data
head(data)
```


## Step 2: Ensure all Variables are Classified Correctly
```{r}
# change all of the character variables to factor variables, the numeric variables can stay numeric
data[sapply(data, is.character)] <- lapply(data[sapply(data, is.character)], as.factor)

str(data)  # output the structure of the data
```

## Step 3: Check for Correlated Variables

We do not need to check for correlated variables because correlation does not impact decision trees.  Decision trees make greedy, localized decisions that are not dependent on previous steps or other variables in the tree model.  
```{r}
#3 Don't check for correlated variables....because it doesn't matter with 
# Decision Trees...the make local greedy decisions. 
```


## Step 4: Standardize Data

We also do not need to standardize the variables.  Decisions in the model are made locally with one variable in focus so the scales of different variables do not need to be standardized.

```{r}
#4 Guess what, you also don't need to standardize the data, 
#because DTs don't give a ish, they make local decisions...keeps getting easier
```

## Step 5: Calculate the Prevalence

Prevalence: 
```{r}
#5 Determine the baserate or prevalence for the classifier, 
# what does this number mean?  

prevalence <- 1-table(data$`salary`)[[1]]/length(data$`salary`)  # calculate the proportion of salary that is the positive class 
prevalence

```

The prevalence is the proportion of the positive class within the target variable, in this case, the proportion of salaries above 50K.  In this data set, the prevalence is roughly 25%.  This means that roughly 75% of the data is the negative class, or salaries below 50K.  This metric can be used a baseline because a model that always predicts the negative class will be correct 75% of the time.  Therefore, we want our model to have an accuracy greater than 75% as this indicates added benefit compared to a model that defaults to predicting the negative class.

## Step 6: Split Data into Train, Tune, and Test Sets

```{r}
#6 Split your data into test, tune, and train. (70/15/15)
set.seed(1)
part_index_1 <- caret::createDataPartition(data$`salary`,  # split the data with a .7 probability so that 70% of the data is chosen
                                           times=1,
                                           p = 0.70,
                                           groups=1,
                                           list=FALSE)

train <- data[part_index_1, ]  # subset the 70% chosen in the first partition into the train set
tune_and_test <- data[-part_index_1, ]  # subset the remaining 7 in a tune and test set 
set.seed(1)
tune_and_test_index <- createDataPartition(tune_and_test$salary,  # now split the tune and test set 50-50
                                           p = .5,
                                           list = FALSE,
                                           times = 1)

tune <- tune_and_test[tune_and_test_index, ]  # subset the 50% chosen into the tune set
test <- tune_and_test[-tune_and_test_index, ]  # subset the remaining 50% into the test set

dims <- data.frame("Train Size" = nrow(train), "Tune Size" = nrow(tune), "Test Size" = nrow(test))  # create a dataframe of the sizes of each set and output the dataframe
dims

```


## Step 7:  Build the Model with the Training Set and Default Settings in Caret

```{r}
features <- train[,c(-15)] # dropping salary column to get just the explanatory variables in features
target <- train$salary  # add just the salary column to target 


fitControl <- trainControl(method = "repeatedcv",  # use repeated cross validation with 5 folds and 3 repeats
                          number = 5,
                          repeats = 3, 
                          returnResamp="all",
                          classProbs = TRUE,
                          allowParallel = TRUE) 

grid <- expand.grid(.winnow = c(TRUE,FALSE), # try with reducing the feature space and without
                    .trials=c(1,5,10,15,20), # number of boosting iterations to try
                    .model="tree")  # use a decision tree model


set.seed(1984)  # set seed for reproducibility
mdl <- train(x=features,  # train the model with the features to predict the target, salary
                y=target,
                method="C5.0",  # use C5.0 model that works by splitting the tree based on maximum info gain
                tuneGrid=grid,
                trControl=fitControl,
                verbose=TRUE)


#provides us the hyper-parameters that were selected through the grid search process. 
mdl # trials = 20 and winnow = FALSE
```

## Step 8: Variable Importance 

```{r}
#8 View the results, what is the most important variable for the tree? 
set.seed(1)
varImp(mdl)  # output the importance of each variable in learning the target variable on a scale from 0 to 100

```
The variables with greater importance have a greater importance measure up to 100 while the variables with lesser importance have a lower measure down to 0.  In the model predicting salaries above and below 50K, the metrics with the greatest importance include capital gain, marital status, and age while the variable with the least important is sex.  


## Step 9: Plot Output of Model Training

```{r}
# visualize the re-sample distributions with different winnows and trials
xyplot(mdl,type = c("g", "p", "smooth"))

```

From the model training output, we can see that the model with no winnow and 20 trails had the greatest accuracy, so this will be the selected model moving forward.  

## Step 10: Predict using Model on the Tune Set

```{r}
#10 Use the validation set and the predict function with your model to the
# estimate the target variable.
set.seed(1989)  # set seed 
pred_tune = predict(mdl,tune)  # predict using the model on the tune set

pred_tune_prob <- predict(mdl, tune, type = "prob")  # save the raw probabilities for each observation being in the positive or negative class

probs_and_results <- cbind(pred_tune_prob, data.frame(`actual salary`= tune$salary))

head(probs_and_results)

```

## Step 11: Predicted vs Actual Target Matrix

```{r}
#11 Compare the predicted values to those of the actual by generating a 
# matrix ("by-hand").

table(as.factor(pred_tune), as.factor(tune$salary))  # make a table of the predicted values and the actual salary values from the tune set

```

It appears that the two majority groups are those that are correctly classified, while the two smaller groups are those that were incorrectly predicted classes.  It also appears that the below50K class is more correctly predicted than the above50K class which is likely due to the greater prevalence of below50K observations in the data set which our model was trained on.

## Step 12: Confusion Matrix with Model on Tune Set

```{r}
#12 Use the the confusion matrix function to check a variety of metrics 
# and comment on the metric that might be best for this type of analysis given your question.  
set.seed(1989) 
eval <- confusionMatrix(as.factor(pred_tune),   # confusion matrix on the predictions and actual values 
                as.factor(tune$salary),
                positive='above50K',  # assign the positive class 
                dnn=c("Prediction", "Actual"), 
                mode = "everything")  # output all evaluation metrics 


eval
```

Overall, it appears that the model is skewed in its ability to predict the positive, above50K, and negative, below50K classes.  The overall accuracy seems high around 0.87.  This is greater than our baseline prevalence of 0.75 which indicates that our model does have an added benefit compared to a model that defaults to predicting the more prevalent target class.  However, our metric of interest is the F1 score rather than accuracy because it takes target class imbalance into consideration.  F1 Score uses the harmonic mean to evaluate accuracy which more sensitive to imbalances between precision and sensitivity that can result from an unbalanced target class as we have in our data set.  The F1 score in our model is 0.73 suggesting that the model does not predict the positive and negative class equally well.  This is also depicted in the Actual vs Prediction matrix as 3,179 out of 3,398 below50K salaries were predicted correctly while only 776 out of 1,126 above50K salaries were predicted correctly.  


## ROC and AUC 

ROC Curve:
```{r}
#13 Generate a ROC and AUC output, interpret the results

# Put predictions and targets in one table 
pred_tune_tibble <- tibble(pred_class=pred_tune, pred_prob=pred_tune_prob$`above50K`, target=as.numeric(tune$salary))

pred <- prediction(pred_tune_tibble$pred_prob, pred_tune_tibble$target) # use predicted prob and target at different threshold levels to build ROC curve

knn_perf <- performance(pred,"tpr","fpr") # prediction for True Pos Rate and False Pos Rate 

plot(knn_perf, colorize=TRUE) # plot ROC curve
```

The ROC curve displays the trade-off between specificity and sensitivity graphically by plotting points that correlate to true and false positive rates at different threshold levels.  If our model had no predictive value, we would see a linear line along the diagonal, y=x.  However, because we see a curved line that gets relatively close to the top-left corner of our plot, we can conclude that our model does have predictive power in determining salary.  


AUC: 
```{r}
tree_perf_AUC <- performance(pred,"auc")  # calculate auc on the predictions

tree_perf_AUC@y.values

```

The area under the ROC curve, abbreviated as AUC, indicates the probability that a positive class observation has a greater predicted probability than a negative class predicted probability.  We want this metric to be high as positive class observations should have higher predictive probabilities than negative class observations.  Therefore, our AUC value of 0.92, close to 1, is indication of a beneficial model.  

## Step 14: Evaluate Model with Different Threshold Levels 

```{r}
#14 Use the predict function to generate percentages, then select several 
# different threshold levels using the confusion matrix function and 
# interpret the results. What patterns did you notice, did the evaluation metrics change? 

# function to take predictions, actual values, and threshold values and then output confusion matrix with those inputs
adjust_thres <- function(x, y, z) {
  set.seed(1989)
  #x=pred_probablities (continuous probs, not 1's and 0's), y=threshold, z=test_outcome
  thres <- as.factor(ifelse(x > y, "above50K","below50K"))
  confusionMatrix(as.factor(thres), as.factor(z), positive='above50K', dnn=c("Prediction", "Actual"), mode = "everything")
}

```

Threshold of 0.4:
```{r}
adjust_thres(pred_tune_prob$`above50K`,.4, tune$salary)  # evaluate with 0.4 threshold 
```

Threshold of 0.6:
```{r}
adjust_thres(pred_tune_prob$`above50K`,.6, tune$salary) # evaluate with 0.6 threshold 
```

By adjusting the model's threshold 0.10 in either direction, we do not see a dramatic change in the model's overall accuracy as both the 0.4 and 0.6 threshold models still have an accuracy around 0.87.  However, we do see a decent shift in F1 Score when using the different threshold values.  When decreasing to 0.4, the model predicts more salaries to be above 50K which helps with the imbalance of predictions present in the first model.  This has increased the F1 score slightly to 0.734.  Alternatively, when increasing to 0.6, the model predicts more salaries to be below 50K, thus furthering the prediction imbalances and decreasing the F1 Score to around 0.7. Because I want the model to be more balanced in its predictions and am trying to optimize F1 Score, I will continue my analysis with the 0.4 threshold which has increased the predictive capabilities for above 50K salaries and has not greatly decreased the predictive capabilities for the below 50K salaries. 

## Step 15: Evaluate Model with Different Hyper-Parameters

```{r}
#15 Based on your understanding of the model and data adjust several of the hyper-parameters via the built in train control function in caret or build and try new features, does the model quality improve? If so how and why, if not, why not?
# Use this link: https://rdrr.io/cran/caret/man/trainControl.html to select changes,
# you aren't expected to understand all these options but explore one or two and 
# see what happens. 

fitControl2 <- trainControl(method = "LGOCV",  # Use leave group out cross validation with 5 folds
                          number = 5,
                          returnResamp="all",
                          classProbs = TRUE,
                          allowParallel = TRUE) 

grid2 <- expand.grid(.winnow = c(TRUE,FALSE), # try with reducing the feature space and without
                    .trials=c(1,5,10,15,20,25,30,40),   # number of boosting iterations to try - add greater values
                    .model="tree") # use a decision tree model


set.seed(1984)  # set seed for reproducibility
mdl2 <- train(x=features,  # train the model again with the new hyper-parameters
                y=target,
                method="C5.0",
                tuneGrid=grid2,
                trControl=fitControl2,
                verbose=TRUE)


set.seed(1989) # set seed for reproducibility
pred_tune2 = predict(mdl2,tune, type= "prob")$`above50K` # predict with the new model on the tune data
set.seed(1)

thres_tune2 <- as.factor(ifelse(pred_tune2 > 0.4, "above50K","below50K"))  # compare the predictions to the 0.4 threshold

eval2 <- confusionMatrix(as.factor(thres_tune2),   # evaluate the new predictions from the new model
                as.factor(tune$salary),
                positive='above50K',
                dnn=c("Prediction", "Actual"), 
                mode = "everything")  # output all of the evaluation metric 


eval2
```

The hyper-parameters altered were the training method, from repeated cross-validation to leave-group-out cross-validation, and adding 25, 30, and 40 as possible boosting iterations.  With these alterations, my intention is that leave-group-out cross-validation will be more thorough in going through all of the observations than the repeated cross-validation method.  Furthermore, I am adding larger numbers of boosting iterations because in the first model training, it appeared as though model accuracy began to increase with increasing boosting iterations.

Analyzing this new model, it appears that these alterations have made a slight improvement.  The F1 Score and overall accuracy are above, but within within 1% of, their respective scores in the initial model with a 0.4 threshold.  Furthermore, it appears that the only change to the Actual vs Prediction matrix is an increase in below 50K predictions for observations that are both actually below 50K and those that are not. This slightly greater lean towards below 50K predictions is likely because the more thorough training has further increased the model's below 50K salary training as it comprises 75% of the data set. Overall, while these adjustments have not greatly improved the model, the F1 score has increased to 74%, so I will continue with this new model at a 0.4 threshold.  

## Step 16: Predict on the Test Set using the Final Model

```{r}
#16 Once you are confident that your model is not improving, via changes 
# implemented on the training set and evaluated on the the validation set (item 16), predict with the test set and report a final evaluation of the model. Discuss the output in comparison with the previous evaluations.  
set.seed(1)
pred_test = predict(mdl2,test, type= "prob")$`above50K`  # predict with the second model on the test set 

thres_test <- as.factor(ifelse(pred_test > 0.4, "above50K","below50K"))  # compare the predictions to the 0.4 threshold

set.seed(1)
confusionMatrix(as.factor(thres_test),  # Use the confusion matrix to evaluate how the model performed on the test set 
                as.factor(test$salary), 
                dnn=c("Prediction", "Actual"), 
                positive='above50K',
                mode = "everything")

```

The final model performs similarly on the test set as it does the tune set with an F1 score around 73% and an overall accuracy of around 86%. The consistency is a good sign that our model is not over-fit, and that it can be generalized to new data sets with a similar performance. Overall, the model does a decent job in predicting whether a salary will be above or below 50K, and its accuracy measure indicates that it does have added benefits compared a model that defaults to predicting the more prevalence target class, below 50K.  However, the F1 Score of around 73% indicates that the model does have an imbalance in predictive ability towards the negative and positive target classes.  This is evident in the confusion matrix where almost 90% of the actual below 50K salaries were correctly predicted, while less than 80% or the actual above 50K salaries were correctly predicted.  

## Step 17: Summary and Recommendations

Throughout this lab, I have learned more about the way that models, specifically decision trees, are formed.  For example, I now understand hyper-parameter inputs such as winnow, whether or not to reduce the feature space, and trials, the number of boosting iterations.  I have also learned more about the concept of repeated cross-validation with different folds and alternatives such as leave-one-out cross-validation.  Generally, in terms of decision trees, I have learned that they make localized decisions to optimize the current inputs, and that they make these decisions by determining what questions result in the greatest information gain.  

Moving forward, I recommend training the model more on above 50K salary data before implementing the model in a real-world situation.  While the model should predict salaries correctly around 86% of the time, this percentage of correct prediction is decently diminished when the salary is above 50K.  Further, given that finances can be a sensitive and powerful subject in the real-world, it is important that these predictions have a smaller error rate than the model currently has.  

## Step 18: Most Interesting or Challenging Part and Questions

During model training, I found two items to be particularly challenging.  The first was the extreme imbalance of the target variable.  I felt that the model did a fantastic job learning the below 50K class, but really struggled in learning, and then predicting, the above 50K class.  I believe a more balanced data set would greatly benefit this model's training and predictive abilities.  The second item I found to be challenging was finding effective hyper-parameter alterations.  While trying to change many of the hyper-parameter inputs, I found that they had little impact of the resulting model's predictions.  I am not sure whether this is due to such thorough initial model training or if I do not yet know what hyper-parameters to alter in certain scenarios.  Given this, my questions are: what are the most influential hyper-parameters? And are there any hyper-parameters that can address such an imbalanced target variable?
