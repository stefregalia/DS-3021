---
title: "Evaluation Metrics Lab"
author: "zcp7yd"
date: "10/27/2021"
output:
  html_document:
    toc: yes
    theme: cosmo
    toc_float: yes
  pdf_document:
    toc: yes
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, error = FALSE, message=FALSE)
```

```{r}
library(caret)
library(tidyverse)
library(class)
library(plotly)
library(MLmetrics)
```

# Metrics Evaluation Lab

## Part 1
### Problem Statement
We will use the kNN algorithm to classify whether a tumor is benign or not based on tumor attributes such as mean radius,texture, perimeter, area, smoothness, compactness, concavity, and concave points. These tumor attributes can be measured in most patients, and if a good KNN model can accurately diagnose malignant tumors, it could help save lives by improving diagnoses and their quickness and efficiency for early prevention and treatment of cancer.
### Importing Dataset
```{r}
tumors <- read_csv('C:/Users/zoe0p/OneDrive/Desktop/DS-3001-zcp7yd/08_IntroML_KNN/tumors.csv')
str(tumors)
view(tumors)
tumors <- tumors[1:32]
view(tumors)
#Scale the features we will be using for classification 
tumors[, c("radius_mean","texture_mean", "perimeter_mean", "area_mean", "smoothness_mean", "compactness_mean", "concavity_mean", "concave points_mean")] <- lapply(tumors[, c("radius_mean","texture_mean", "perimeter_mean", "area_mean", "smoothness_mean", "compactness_mean", "concavity_mean", "concave points_mean")],function(x) scale(x))

view(tumors)

```



## Part 2 

### Key Metrics
We chose 3 key evaluation metrics from the tumors dataset to track in order to determine tumor diagnosis: **sensitivity or true positive rate, accuracy, and false positive rate**. We chose sensitivity and accuracy because these two qualities record important information differentiating tumor diagnoses, which are of high importance because it impacts humans' lives. False positive rate was chosen because it measures inaccurate malignant diagnoses, and we want to track and minimize inaccurate positive diagnoses to reduce patients' costs and treatment and save resources for actual positive cases in this high stakes problem.

## Part 3

### kNN Model Building

#### Data Partition
```{r}

# Data partition the tumors dataset in half to create train and test data sets

part_index_1 <- createDataPartition(tumors$diagnosis,
                                           times=1,
                                           p = 0.50,
                                           groups=1,
                                           list=FALSE)
View(part_index_1)

train <- tumors[part_index_1,]
test <- tumors[-part_index_1, ]

dim(train)
dim(test)

```
#### Train the classifier
```{r}


# Let's train the classifier for k = 3 using the class package. 


# k-Nearest Neighbor is a randomized algorithm, so make sure to use set.seed() to make your results repeatable.
set.seed(100)
tumors_3NN <-  knn(train = train[, c("radius_mean","texture_mean", "perimeter_mean", "area_mean", "smoothness_mean", "compactness_mean", "concavity_mean", "concave points_mean")],#<- training set cases
               test = test[, c("radius_mean","texture_mean", "perimeter_mean", "area_mean", "smoothness_mean", "compactness_mean", "concavity_mean", "concave points_mean")],    #<- test set cases
               cl = train$diagnosis,#<- category for true classification
               k = 3,#<- number of neighbors considered
               use.all = TRUE,
               prob = TRUE) #<- control ties between class assignments If true, all distances equal to the kth largest are included



# View the output.
str(tumors_3NN)
table(tumors_3NN)
length(tumors_3NN)
```

## Part 4

### Compare to Original Data

```{r}
# How does the kNN classification compare to the true class?
# Let's take a look at the confusion matrix by combining the predictions from tumors_3NN to the original data set.
kNN_res = table(tumors_3NN,
                test$diagnosis)
kNN_res
sum(kNN_res)  #<- the total is all the test examples

# Select the true positives and true negatives by selecting
# only the cells where the row and column names are the same.
kNN_res[row(kNN_res) == col(kNN_res)]

# Calculate the accuracy rate by dividing the correct classifications
# by the total number of classifications.
kNN_acc = sum(kNN_res[row(kNN_res) == col(kNN_res)]) / sum(kNN_res)

kNN_acc
# A 95% accuracy is very good prediction of tumor diagnosis, probably due to the tumor size correlating closely to tumor diagnosis.

set.seed(100)
confusionMatrix(as.factor(tumors_3NN), as.factor(test$diagnosis), positive = "M", dnn=c("Prediction", "Actual"), mode = "sens_spec")

```
My KNN model where k=3 classifying malignant tumors as the 'positive' case produces a 93% accuracy which is pretty good prediction of tumor diagnosis, probably due to the tumor size correlating closely to tumor malignancy, with larger sizes tending to be malignant. The sensitivity or true positive rate is 92%, which is also very high, which is good in that our model predicts many true positives and diagnoses malignant tumors correctly a majority of the time. The false positive rate is calculated to be 1-Specificity, or 1-0.9382=0.0618 or 6.1%, which is very low, meaning that this current model produces a low rate of inaccurately diagnosing benign tumors as malignant. 

### Missclassification Errors: Patterns
Mis-classification errors seem to be occurring when larger tumors are not malignant because while there is a usual positive relationship, tumor sizes can vary based on location and type of cancer. This is reflected in the high sensitivity, accuracy, and low false positive rate, while there still being some mis-classifications (less than 90%).


## Part 5 
### Threshold Adjustment
```{r}


adjust_thres <- function(x, y, z) {
  #x=pred_probablities, y=threshold, z=test_outcome
  thres <- as.factor(ifelse(x > y, 1,0))
  confusionMatrix(thres, z, positive = "1", dnn=c("Prediction", "Actual"), mode = "everything")
}

tumor_eval_prob <- attr(tumors_3NN, "prob")

adjust_thres(tumors_eval_prob$diagnosis,.60, test$diagnosis) 


```

After increasing the thresholds for diagnosing a tumor as malignant, I saw slight differences across my evaluation metrics: accuracy decreased, true positive rate increased, and false positive rate decreased. This means that my model was less accurate in identifying tumors correctly, but correctly identified more actually malignant tumors and didn't identify false positives as much. This model behavior can be attributed to increasing the thresholds for malignant tumor diagnosis, as a larger size of tumor will be diagnosed as malignant as opposed to the earlier iteration of my model. While the increased true positive and decreased false positive rate shows an improvement of my model, increasing the threshold also decreases my model accuracy and false negatives, which is harmful for patients in not diagnosing malignant tumors properly for treatment in time. 

## Part 6

### Findings and Recommendations

By using a KNN model, I was able to produce a highly accurate, sensitive, and specific model to predict tumor malignancy based on mean tumor sizes (radius,texture, perimeter, area, smoothness, compactness, concavity, and concave points). My model had an accuracy of 93%, a true positive rate of 92%, and a 6.1% false positive rate, showing it accurately diagnosed tumors and correctly diagnosed malignant tumors a large majority of the time, while falsely identifying benign tumors as malignant very few times. These accurate and correct diagnoses means malignant tumors are correctly diagnosed a majority of the time based on tumor size, saving patients' lives by early and quick diagnosis and prevention, while a low false positive rate saves patients' costs and alleviates them of worries by misdiagnosing a benign tumor as malignant. 

Recommendations for improvements on this KNN model may include gathering more data to get more accurate predictions, lowering the threshold to reduce the false negative rate, adding more variables or tumor attributes to predict on as tumor malignancy is affected by measurable factors other than size, and possibly increasing k or the number of neighbors for the KNN model to provide a sufficient majority to vote from given the different factors in tumor size contribute to malignancy in different ways.

